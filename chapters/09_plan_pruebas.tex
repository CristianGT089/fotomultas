\section{Plan de pruebas}

\subsection{Concepto de prueba}
En el contexto de este proyecto, una prueba de software se entiende como un \textit{proceso sistemático de evaluación} mediante el cual se ejecutan componentes o funcionalidades del prototipo bajo condiciones controladas, con el propósito de observar su comportamiento y compararlo frente a resultados esperados previamente definidos. Una prueba no se reduce a “probar si funciona”, sino que busca evidenciar si el sistema cumple o no con requisitos funcionales y no funcionales específicos (integridad, trazabilidad, tiempos de respuesta, manejo de errores), a partir de criterios de aceptación claros y repetibles.

Desde la ingeniería de software, una prueba está compuesta por un conjunto de casos de prueba que describen: (i) las precondiciones del escenario, (ii) los datos o acciones que se aplican al sistema y (iii) los resultados esperados y observados. En este plan, las pruebas se orientan a validar la hipótesis central del trabajo: que un prototipo basado en tecnologías de registro distribuido puede garantizar inmutabilidad, verificabilidad y desempeño aceptable en la gestión de fotocomparendos.

\subsection{Propósito del plan}
El propósito de este plan es guiar la evaluación de la efectividad y viabilidad del prototipo desarrollado para la gestión de fotocomparendos utilizando Hyperledger Fabric e IPFS. Se busca validar que el prototipo cumple con los requisitos clave de inmutabilidad, transparencia, seguridad, y medir su rendimiento básico, comparándolo con las limitaciones identificadas en el sistema tradicional de Bogotá.

\subsection{Alcance de las pruebas}
\begin{itemize}
    \item Proceso completo de registro de un fotocomparendo: captura simulada, carga de evidencia a IPFS, registro de metadatos y hash IPFS en el ledger.
    \item Consulta y verificación de fotocomparendos registrados.
    \item Verificación de la inmutabilidad de los registros en el ledger y de la evidencia en IPFS.
    \item Consistencia de los datos entre la UI, el ledger y IPFS.
    \item Rendimiento básico de operaciones clave (registro, consulta).
    \item Actualización del estado de la multa (ej. "Pagada", "Apelada").
\end{itemize}

\subsection{Fuera de alcance}
\begin{itemize}
    \item Pruebas de estrés o carga exhaustivas.
    \item Pruebas de penetración de seguridad avanzadas.
    \item Integración completa con sistemas externos reales (RUNT, SIMIT) más allá de APIs simuladas o de prueba.
    \item Pruebas de usabilidad exhaustivas con usuarios finales.
    \item Funcionalidad de pago automatizado con billetera digital.
\end{itemize}

\subsection{Entorno de pruebas}
\paragraph{Hardware}
\begin{itemize}
    \item Servidor(es) para nodos Hyperledger Fabric (pueden ser VMs o contenedores Docker). 
    \item Servidor(es) para nodo(s) IPFS (pueden ser VMs o contenedores Docker). 
    \item Máquina para ejecutar la aplicación backend (Node.js/Express según requisitos del sistema). 
    \item Máquinas cliente para acceder a la interfaz web (simulando Agente de Movilidad y Ciudadano).
\end{itemize}
\paragraph{Software}
\begin{itemize}
    \item Hyperledger Fabric v2.5 (versión específica utilizada en el prototipo).
    \item IPFS Kubo v0.34.1 (versión específica utilizada en el prototipo).
    \item Base de datos (si la aplicación backend la usa adicionalmente). 
    \item Aplicación backend (Node.js, Express, etc.).
        \item Aplicación frontend (navegador web). 
    \item Herramientas de monitoreo y logging.
\end{itemize}
\paragraph{Datos de prueba}
\begin{itemize}
    \item Conjunto de imágenes de evidencia (JPG, PNG) de diferentes tamaños. 
    \item Datos de fotocomparendos ficticios (placas, fechas, ubicaciones, tipos de infracción). 
    \item Datos de usuarios simulados (Agentes de Movilidad, Administradores, Ciudadanos).
\end{itemize}

\subsection{Tipos de pruebas y casos}

\input{tables/casos_prueba_funcionales}

\noindent En la Tabla~\ref{tab:casos_prueba_funcionales} se enumeran los casos de prueba funcionales definidos para verificar el comportamiento básico del sistema, desde el registro de un fotocomparendo hasta la validación de su integridad y actualización de estado. Cada caso detalla las precondiciones, las acciones a ejecutar y el resultado esperado, sirviendo como guía para las pruebas manuales y automatizadas.

\subsection{Pruebas de inmutabilidad}

\input{tables/casos_prueba_inmutabilidad}

\noindent La Tabla~\ref{tab:casos_prueba_inmutabilidad} detalla los escenarios diseñados para poner a prueba la inmutabilidad del sistema ante intentos de modificación no autorizada, mientras que la Tabla~\ref{tab:resultados_inmutabilidad} resume los resultados obtenidos en dichas pruebas, evidenciando la correcta detección y rechazo de cambios indebidos.

\subsection{Pruebas de rendimiento}
Se medirá el tiempo requerido para ejecutar operaciones clave en condiciones simuladas de uso real. Los tiempos objetivo son:

\begin{itemize}
    \item Registro de fotocomparendo: $\leq$ 3 segundos
    \item Consulta de fotocomparendo: $\leq$ 1 segundo
    \item Verificación de integridad (hash IPFS): $\leq$ 2 segundos
    \item Actualización de estado: $\leq$ 2 segundos
\end{itemize}

Los resultados detallados de estas pruebas se presentan en la sección Resultados de las pruebas de inmutabilidad y verificabilidad del prototipo.

\subsection{Casos de prueba funcionales}

\input{tables/casos_prueba_inmutabilidad_verificabilidad.tex}

\subsection{Pruebas de interfaz de usuario}

Para validar la funcionalidad de la interfaz de usuario, se implementó una suite de pruebas automatizadas utilizando Jest y React Testing Library. La estrategia contempló:

\begin{itemize}
    \item \textbf{Pruebas unitarias:} Verificación del renderizado y comportamiento de componentes individuales (botones, formularios, tablas de datos).
    \item \textbf{Pruebas de integración:} Validación de flujos completos de usuario, incluyendo autenticación, gestión de multas y consulta pública.
    \item \textbf{Casos de borde:} Manejo de entradas inesperadas, errores de red y responsividad.
\end{itemize}

Se ejecutaron aproximadamente 58 pruebas automatizadas, alcanzando una cobertura de código superior al 90\% en componentes críticos. Los resultados detallados se presentan en la sección Resultados de las pruebas de inmutabilidad y verificabilidad del prototipo.
